# 자연어 크롤링 및 텍스트 데이터 처리

## 크롤링 방식

- Selenium보다 requests를 사용하는 것이 속도 면에서 유리하다.
- 크롤링 대상에서 링크만 추출하여 pandas 등으로 가공하면 효율적이다.
- 무한 스크롤 페이지는 while문으로 스크롤 이벤트를 시뮬레이션하여 처리 가능하다.

## 텍스트 데이터 처리

- 대부분의 크롤링 데이터는 로그 포함 텍스트 기반이므로 정규표현식 활용 능력이 중요하다.
- `.pkl` 파일 포맷은 Python 객체를 바이너리로 저장하는 형태로, 데이터 가공/저장/재사용에 활용된다.

## 뉴스 크롤링 전략

- 같은 뉴스라도 접근 방식을 잘 설계하면 수집이 훨씬 수월해질 수 있다.

## 댓글 데이터 수집 및 전처리

- 맞춤법 오류, 비문 등이 많아 수집 자체가 어렵다.
- 구어체와 문어체는 교정 방식이 다르므로 분리하여 처리해야 한다.
- 완벽한 정제는 수작업이 필요한 경우가 많다. (GIGO 방지 목적)

## 형태소 분석

- 명사, 고유명사, 동사, 형용사 중심의 분석이 중요하다.
- 사용자 사전에 고유명사를 추가하여 분석 품질을 향상시킬 수 있다.

## 형태소 기반 매칭 및 검증

- 댓글 원문과 주요 형태소만 추출된 문장을 매치하여 비교한다.
- 누락된 단어는 사용자 사전에 추가하고, 최종 결과를 수작업으로 재검토한다.

## 텍스트 빈도 분석

- TF(Term Frequency): 문서 내 단어 등장 횟수
- IDF(Inverse Document Frequency): 특정 단어가 여러 문서에 얼마나 희소한지를 나타냄
  - 공식: log(Nd / Ndf)
  - Nd: 전체 문서 수
  - Ndf: 특정 단어가 등장한 문서 수
  - IDF 값이 높을수록 특정 문서에만 등장한 특징적인 단어임
- TF-IDF는 이 둘을 곱해 단어의 중요도를 계산하는 방식이다.

## 연관 단어 시각화

- 동시 등장한 단어들을 연결하여 연관성을 시각화한다.

## LDA 토픽 모델링

- 문서 집합을 여러 개의 주제로 분해하여 문서의 구조를 파악할 수 있다.
- 단어의 등장 빈도가 주제를 결정한다.
- 일반적으로 10개 미만의 토픽 수로 설정하며, 각 주제에 따라 실행 전략(Action Plan)을 수립한다.
- 등장 빈도가 높다고 해서 중요도가 높은 것은 아니므로, 출현 빈도와 중요도는 구분해야 한다.

## 결론

- 텍스트 전처리는 많은 리소스를 요구하지만, 전체 분석 결과의 품질을 결정짓는 핵심 단계이다.