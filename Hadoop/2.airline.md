# Airline dataset
하둡에 데이터 올려서 불러와서 조회해봄
```
%pyspark
file_path = 'hdfs://localhost:9000/input/airline'
df = spark.read.csv(file_path, header=True, inferSchema=True)
```
1. `header` : csv 가장 첫줄에 헤더를 정의하고 있으면 컬럼명이 정의한대로 지정됨
2. `inferSchma` : 데이터값에 따라 type을 자동지정(안되는 컬림이 발생하는 경우도 있음)
## `select`
필요한 컬럼만 선택하여 새로운 dataframe 작성
```
%pyspark
from pyspark.sql.functions import col
df = df.select(
    'Month',
    'DayofMonth',
    'DayofWeek',
    'Origin',
    'Dest',
    'Cancelled',
    'UniqueCarrier',
    'FlightNum',
    col('AirTime').cast('int'),
    col('ArrTime').cast('int'),
    col('ArrDelay').cast('int'),
    col('DepTime').cast('int'),
    col('DepDelay').cast('int'),
    col('ActualElapsedTime').cast('int'),
    col('CRSElapsedTime').cast('int'),
    )
```
- `col`, `cast`로 계산하기 쉽게 타입을 수동지정
## view
```
%pyspark
df.createOrReplaceTempView('airline')
```
- 이제부터 sql로 데이터에 접근/조회 가능
## 문제
1. 요일별 출발/도착 지연 시간의 평균
```
%sql
SELECT DayofWeek, AVG(ArrDelay), AVG(DepDelay)
FROM airline
GROUP BY DayofWeek;
```
```
%pyspark
df.groupBy('DayofWeek').agg(avg('ArrDelay'), avg('DepDelay'))
```

2. 항공사별 취소율(==취소된 횟수/계획 비행횟수)
이중 쿼리 구조 : 서브쿼리(Subquery)라고도 부른다. SELECT부터 시작된 하나의 쿼리문은 결과가 하나의 테이블로 출력되므로 출력된 테이블 바깥에 SELECT FROM으로 감싸서 사용이 가능하다. 
```
%sql
SELECT
    *,
    (flight_cancel / total * 100) AS cancel_rate
FROM
(SELECT  UniqueCarrier, SUM(Cancelled) AS flight_cancel,
    sum(CASE WHEN Cancelled == 0 THEN 1 ELSE 0 END), COUNT(*) AS total
    FROM airline
group by UniqueCarrier)
```
```
%pyspark
df.groupBy('UniqueCarrier')\
    .agg(
        sum('Cancelled').alias('flight_cancelled_count'),
        sum(when(df.Cancelled == 0, 1).otherwise(0)),
        count('*').alias('total_count'),
    ).withColumn('cancel_rate', col('flight_cancelled_count') / col('total_count')*100).show()
```
