# Airline dataset
하둡에 데이터 올려서 불러와서 조회해봄
```
%pyspark
file_path = 'hdfs://localhost:9000/input/airline'
df = spark.read.csv(file_path, header=True, inferSchema=True)
```
1. `header` : csv 가장 첫줄에 헤더를 정의하고 있으면 컬럼명이 정의한대로 지정됨
2. `inferSchma` : 데이터값에 따라 type을 자동지정(안되는 컬림이 발생하는 경우도 있음)
## `select`
필요한 컬럼만 선택하여 새로운 dataframe 작성
```
%pyspark
from pyspark.sql.functions import col
df = df.select(
    'Month',
    'DayofMonth',
    'DayofWeek',
    'Origin',
    'Dest',
    'Cancelled',
    'UniqueCarrier',
    'FlightNum',
    col('AirTime').cast('int'),
    col('ArrTime').cast('int'),
    col('ArrDelay').cast('int'),
    col('DepTime').cast('int'),
    col('DepDelay').cast('int'),
    col('ActualElapsedTime').cast('int'),
    col('CRSElapsedTime').cast('int'),
    )
```
- `col`, `cast`로 계산하기 쉽게 타입을 수동지정
## view
```
%pyspark
df.createOrReplaceTempView('airline')
```
- 이제부터 sql로 데이터에 접근/조회 가능
## 문제
1. 요일별 출발/도착 지연 시간의 평균
```
%sql
SELECT DayofWeek, AVG(ArrDelay), AVG(DepDelay)
FROM airline
GROUP BY DayofWeek;
```
```
%pyspark
df.groupBy('DayofWeek').agg(avg('ArrDelay'), avg('DepDelay'))
```

2. 항공사별 취소율(==취소된 횟수/계획 비행횟수)
이중 쿼리 구조 : 서브쿼리(Subquery)라고도 부른다. SELECT부터 시작된 하나의 쿼리문은 결과가 하나의 테이블로 출력되므로 출력된 테이블 바깥에 SELECT FROM으로 감싸서 사용이 가능하다. 
```
%sql
SELECT
    *,
    (flight_cancelled_count / total_count * 100) AS cancel_rate
FROM
(SELECT
    UniqueCarrier,
    SUM(Cancelled) AS flight_cancelled_count,
    COUNT(*) AS total_count
FROM airline
GROUP BY UniqueCarrier)
```
```
%pyspark
df.groupBy('UniqueCarrier')\
    .agg(
        sum('Cancelled').alias('flight_cancelled_count'),
        count('*').alias('total_count'),
    ).withColumn('cancel_rate', col('flight_cancelled_count') / col('total_count')*100).show()
```
3. 가장 붐비는 공항
```
%sql
SELECT *, origin_count + dest_count AS total
FROM
(
(SELECT Origin, COUNT(*) AS origin_count
FROM airline
GROUP BY Origin) AS origin_airline

JOIN

(SELECT Dest, COUNT(*) AS dest_count
FROM airline
GROUP BY Dest) AS dest_airline

ON origin_airline.Origin == dest_airline.Dest
)
ORDER BY total DESC LIMIT 10
```
```
%pyspark

origin_df = df.groupBy('Origin').count()

dest_df = df.groupBy('Dest').count()

origin_df.join(dest_df, origin_df.Origin == dest_df.Dest).withColumn('total', origin_df['count'] + dest_df['count']).orderBy(desc('total')).show()
```
4. 실제 비행시간과 예상 비행시간 차이가 큰 비행노선
```
%sql
SELECT
    *, ABS(real_time - crs_time) AS diff_time
FROM
(SELECT 
    Origin, 
    Dest, 
    AVG(ActualElapsedTime) AS real_time, 
    AVG(CRSElapsedTime) AS crs_time
FROM airline
GROUP BY Origin, Dest)
ORDER BY diff_time DESC
```
```
%pyspark
df.groupBy('Origin', 'Dest') \
    .agg(
        avg('ActualElapsedTime').alias('real_time'),
        avg('CRSElapsedTime').alias('crs_time')
    ).withColumn('diff_time', abs(col('real_time')-col('crs_time'))) \
    .orderBy(desc('diff_time')).show()
```